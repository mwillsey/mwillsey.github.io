---
layout: paper
title: "babble: Learning Better Abstractions with E-Graphs and Anti-Unification"
shorttitle: babble

type: "publication"
authors:
  - dcao
  - rose
  - cnandi
  - mwillsey
  - ztatlock
  - nadia
venue: "POPL 2023"
venue_url: "https://popl23.sigplan.org/details/POPL-2023-popl-research-papers/14/babble-Learning-Better-Abstractions-with-E-Graphs-and-Anti-Unification"

links:
  arXiv: "https://arxiv.org/abs/2212.04596"
  "ACM DL": "https://dl.acm.org/doi/10.1145/3571207"

tag: conference

bib: |
  @article{babble,
    author = {Cao, David and Kunkel, Rose and Nandi, Chandrakana and Willsey, Max and Tatlock, Zachary and Polikarpova, Nadia},
    title = {Babble: Learning Better Abstractions with E-Graphs and Anti-Unification},
    year = {2023},
    issue_date = {January 2023},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {7},
    number = {POPL},
    url = {https://doi.org/10.1145/3571207},
    doi = {10.1145/3571207},
    journal = {Proc. ACM Program. Lang.},
    month = {jan},
    articleno = {14},
    numpages = {29},
    keywords = {e-graphs, anti-unification, library learning}
  }
---

## Abstract

Library learning compresses a given corpus of programs by extracting
common structure from the corpus into reusable library functions. Prior
work on library learning suffers from two limitations that prevent it
from scaling to larger, more complex inputs. First, it explores too many
candidate library functions that are not useful for compression. Second,
it is not robust to syntactic variation in the input.

We propose library learning modulo theory (LLMT), a new library learning
algorithm that additionally takes as input an equational theory for a
given problem domain. LLMT uses e-graphs and equality saturation to
compactly represent the space of programs equivalent modulo the theory,
and uses a novel e-graph anti-unification technique to find common
patterns in the corpus more directly and efficiently.

We implemented LLMT in a tool named <code>babble</code>. Our evaluation shows that
<code>babble</code> achieves better compression orders of magnitude faster than the
state of the art. We also provide a qualitative evaluation showing that
<code>babble</code> learns reusable functions on inputs previously out of reach for
library learning.
